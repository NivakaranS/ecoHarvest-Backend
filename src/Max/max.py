from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel 
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_groq import ChatGroq
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from chromadb.config import Settings
from langchain_chroma import Chroma
import os
from dotenv import load_dotenv


load_dotenv()
os.environ["HF_TOKEN"] = os.getenv("HF_TOKEN")
os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")

app = FastAPI()

origins = [
    "http://localhost:3000",
    "http://localhost:3000/"
    
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)


embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

llm = ChatGroq(model_name="Deepseek-R1-Distill-Llama-70b")

#Session store
session_store = {}

class QuestionRequest(BaseModel):
    session_id: str
    question: str


def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    splits = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=f"./max.db",
        client_settings=Settings(
            persist_directory=f"./max.db"
        )
    )
    return vectorstore



# Initialize Chroma with pre-loaded PDF
vectorstore = process_pdf('max2.pdf')
retriever = vectorstore.as_retriever()


@app.post("/ask_question/")
async def ask_question(request: QuestionRequest):

    session_id = request.session_id
    question = request.question

    if session_id not in session_store:
        retriever2 = retriever
    else:
        retriever2 = session_store[session_id]["retriever"]
    
    contextualize_q_system_prompt = (
        "Given the chat history and the latest user question"
        "which might reference context in the chat history"
        "formulate a standalone question which can be understood"
        "without the chat history. DO NOT answer the question,"
        "just reformulate it if needed and otherwise return as it is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    history_aware_retriever = create_history_aware_retriever(llm, retriever2, contextualize_q_prompt)

    system_prompt = (
        "You are Max, a AI assistant for providing information about Ecoharvest,"
        "Address him as sir. You are like Jarvis for Tony Stark in iron man"
        "Keep your messages short and precise. Be professional. "
        "You are created to assist him. Say the question is out of scope "
        "ONLY if you are asked questions out of the context of the chat history"
        "Use the following pieces of retrieved context : \n\n{context}" 
    )


    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    if session_id not in session_store:
        session_store[session_id] = {
            "rag_chain" : rag_chain,
            "history": ChatMessageHistory(),
            "retriever": retriever2
        }

    rag_chain = session_store[session_id]["rag_chain"]
    session_history = session_store[session_id]["history"]

    last_6_messages = session_history.messages[-6:]

    try:
        response = rag_chain.invoke(
            {"input": question, "chat_history": last_6_messages},
            config={
                "configurable": {"session_id": session_id}
            }
        )
    except Exception as e:
        print(f"Error: {e}")
        return {"error": str(e)}, 500
 

    if "answer" not in response:
        return {"error": "No answer generated by the model."}, 500

    answer = str(response.get("answer", ""))

    session_history.add_user_message(question)
    session_history.add_ai_message(answer)


    

    end_tag = '</think>' 

    end_tag_pos = answer.find(end_tag)

    if end_tag_pos != -1:
        result = answer[end_tag_pos + len(end_tag):].strip()
    else:
        end_tag = '<think>'
        end_tag_pos = answer.find(end_tag)
        if end_tag_pos != -1:
            result = answer[end_tag_pos + len(end_tag):].strip()
        
        result = answer
        

    return {"answer": result}


@app.get("/")
def home():
    return {"message": "Welcome to Max"}

if __name__ == "__main__":  
    import uvicorn
    uvicorn.run("max:app", host="0.0.0.0", port=5000, reload=True)  # Corrected
